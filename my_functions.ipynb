{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for use throughout the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:<a class=\"anchor\" id=\"contents\"></a>\n",
    "* [Model building and compiling](#modelbuilding)\n",
    "* [Fitting Models to Generators](#fitting)\n",
    "* [Plotting Results](#plotting)\n",
    "* [Miscellaneous](#misc)\n",
    "* [Model Ensembling](#ensembling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras import Input\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "from keras_gcnn.layers import GConv2D, GBatchNorm\n",
    "from keras_gcnn.layers.pooling import GroupPool\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Cropping2D # Need this for concatenate to work with valid pooling\n",
    "from keras.regularizers import l2 # Weight decay from the DenseNet paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building and compiling <a class=\"anchor\" id=\"modelbuilding\"></a>\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This method was adapted from my own course work for the Artificial Intelligence Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(input_layer, hidden_layer_units=None, output = \"sigmoid\",\n",
    "                            img_size=96,labels = 1, opt=\"Adam\",  learn = 1e-3, convpad = \"same\",\n",
    "                            poolpad=\"valid\", metrics = [\"accuracy\"],weight_reg=1e-4):\n",
    "    \"\"\"Takes an input layer unit size and several other optional tuning parameters and returns a compiled Keras model.\n",
    "    \n",
    "    This method automates part of the Keras model building and compiling process, allowing a user to simply\n",
    "    give their hyper-parameters and model characteristics as arguments in one function. It has set defaults \n",
    "    relative to this specific dataset currently.\n",
    "    \n",
    "    It makes use of the functional Keras API rather than the sequential Model class.\n",
    "    \n",
    "    \"\"\"\n",
    "    input_tensor = Input(shape=(img_size,img_size,3))\n",
    "    x = layers.Conv2D(input_layer, (3,3), activation=\"relu\", padding=convpad) (input_tensor)\n",
    "    \n",
    "    if hidden_layer_units is not None:\n",
    "        for u in hidden_layer_units:\n",
    "            if isinstance(u, str):\n",
    "                if \"conv\" in u:\n",
    "                    if \"reg\" in u:\n",
    "                        x = layers.Conv2D(int(u[7:]), (3,3), activation = \"relu\",\n",
    "                                          padding=convpad, kernel_regularizer=regularizers.l2(weight_reg)) (x)\n",
    "                    else:\n",
    "                        x = layers.Conv2D(int(u[4:]), (3,3), activation = \"relu\", padding=convpad) (x)\n",
    "                elif \"dropout\" in u:\n",
    "                    x = layers.Dropout(float(u[7:])) (x)\n",
    "                elif \"maxpool\" in u:\n",
    "                    x = layers.MaxPooling2D((2,2), padding=poolpad) (x)\n",
    "                elif \"globalavg\" in u:\n",
    "                    x = layers.GlobalAveragePooling2D() (x)\n",
    "                elif \"globalmax\" in u:\n",
    "                    x = layers.GlobalMaxPooling2D() (x)\n",
    "                elif \"flatten\" in u:\n",
    "                    x = layers.Flatten() (x)\n",
    "                elif \"norm\" in u:\n",
    "                    x = layers.BatchNormalization() (x)\n",
    "                else:\n",
    "                    if u != \"\":\n",
    "                        print(\"Invalid argument for hidden_layer_units: {}\".format(u))\n",
    "            else:\n",
    "                x = layers.Dense(u, activation = \"relu\") (x)\n",
    "    \n",
    "    output_tensor = layers.Dense(labels, activation = output) (x)\n",
    "    \n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    if output == \"sigmoid\":\n",
    "        l = \"binary_crossentropy\"\n",
    "    else:\n",
    "        l = \"categorical_crossentropy\"\n",
    "    if opt == \"RMS\":\n",
    "        model.compile(optimizer = optimizers.RMSprop(lr = learn),\n",
    "            loss = l,\n",
    "            metrics = metrics)\n",
    "    elif opt == \"Adam\":\n",
    "        model.compile(optimizers.Adam(lr = learn),\n",
    "            loss = l,\n",
    "            metrics = metrics)                \n",
    "    else:\n",
    "        print(\"Invalid argument for 'opt'\")\n",
    "        return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model_GCNN(gconv_type = \"C4\", input_layer = 32, hidden_layer_units=None,\n",
    "                                 output = \"sigmoid\", labels = 1, img_size=96, opt=\"Adam\",  learn = 1e-3,\n",
    "                                 poolpad=\"valid\", metrics = [\"accuracy\"], weight_reg=1e-4):\n",
    "    \"\"\"Takes an input layer unit size and several other optional tuning parameters and returns a compiled Keras model.\n",
    "    \n",
    "    This is adapted from my standard building and compilation function to work solely with non-standard\n",
    "    Group Convolutional Neural Networks (G-CNNs)\n",
    "    \n",
    "    The layers utilised are extended from the built-in Keras layers to exploit rotational and reflectional symmetries.\n",
    "    \n",
    "    gconv_type specifies the specific gconv operation to use.\n",
    "    \"C4\" contains the four pure 90 degree rotations\n",
    "    \"D4\" contains the eight roto-reflections\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    input_tensor = Input(shape=(img_size,img_size,3))\n",
    "    x = GConv2D(input_layer, 3, \"Z2\", gconv_type, activation=\"relu\", padding=\"same\") (input_tensor)\n",
    "    \n",
    "    if hidden_layer_units is not None:\n",
    "        for u in hidden_layer_units:\n",
    "            if isinstance(u, str):\n",
    "                if \"conv\" in u:\n",
    "                    if \"reg\" in u:\n",
    "                        x = GConv2D(int(u[7:]), 3, gconv_type, gconv_type, activation = \"relu\",\n",
    "                                    padding=\"same\", kernel_regularizer=regularizers.l2(weight_reg)) (x)\n",
    "                    else:\n",
    "                        x = GConv2D(int(u[4:]), 3, gconv_type, gconv_type, activation = \"relu\", padding=\"same\") (x)\n",
    "                elif \"dropout\" in u:\n",
    "                    x = layers.Dropout(float(u[7:])) (x)\n",
    "                elif \"maxpool\" in u:\n",
    "                    x = layers.MaxPooling2D((2,2), padding=poolpad) (x)\n",
    "                elif \"grouppool\" in u:\n",
    "                    x = GroupPool(h_input=gconv_type)(x)\n",
    "                elif \"globalavg\" in u:\n",
    "                    x = layers.GlobalAveragePooling2D() (x)\n",
    "                elif \"globalmax\" in u:\n",
    "                    x = layers.GlobalMaxPooling2D() (x)\n",
    "                elif \"flatten\" in u:\n",
    "                    x = layers.Flatten() (x)\n",
    "                elif \"norm\" in u:\n",
    "                    x = GBatchNorm(gconv_type)(x)\n",
    "                else:\n",
    "                    if u != \"\":\n",
    "                        print(\"Invalid argument for hidden_layer_units: {}\".format(u))\n",
    "            else:\n",
    "                x = layers.Dense(u, activation = \"relu\") (x)\n",
    "    \n",
    "    output_tensor = layers.Dense(labels, activation = output) (x)\n",
    "    \n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    if output == \"sigmoid\":\n",
    "        l = \"binary_crossentropy\"\n",
    "    else:\n",
    "        l = \"categorical_crossentropy\"\n",
    "    if opt == \"RMS\":\n",
    "        model.compile(optimizer = optimizers.RMSprop(lr = learn),\n",
    "            loss = l,\n",
    "            metrics = metrics)\n",
    "    elif opt == \"Adam\":\n",
    "        model.compile(optimizers.Adam(lr = learn),\n",
    "            loss = l,\n",
    "            metrics = metrics)                \n",
    "    else:\n",
    "        print(\"Invalid argument for 'opt'\")\n",
    "        return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNet model builder functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_dense_model(filters,growable_filters=None,growth_rate=3,dense_blocks=5,is_gconv=True,gconv_type=\"C4\",conv_layers=1,\n",
    "                                  padding=\"valid\",pooling=\"avg\",dropout=0,norm=True,labels=1,img_size=96,\n",
    "                                  output=\"sigmoid\",opt=\"adam\",learn=1e-3,metrics=[\"accuracy\"],weight_decay=1e-4,\n",
    "                                 bc_model=False):\n",
    "    \n",
    "    \"\"\" Takes as input :the initial filters, growth rate, dense block structure, g_conv type, and several others. \n",
    "    \n",
    "    This method provides the ability to automatically construct DenseNet models, either with standard convolutions\n",
    "    or with group-equivariant layers. It also has the option to include bottleneck layers and compression, as described \n",
    "    in the DenseNet paper.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Firstly defining the dense and transition blocks as nested functions.\n",
    "    \n",
    "    def dense_block(input_layer,filters,growth_rate,is_gconv=True,gconv_type=\"C4\",conv_layers=1,\n",
    "                padding=\"valid\",dropout=0,norm=True,weight_decay=1e-4,bc_model=False):\n",
    "    \n",
    "        \"\"\" \n",
    "        Used by its parent function to create the individual dense blocks for the overall network. \n",
    "        \n",
    "\n",
    "        \"\"\"        \n",
    "        x = input_layer\n",
    "        for i in range(conv_layers):\n",
    "\n",
    "            #if BC model version of DenseNet - add a \"bottleneck\" conv layer\n",
    "            if bc_model:\n",
    "                if norm:\n",
    "                    if is_gconv:\n",
    "                        conv = GBatchNorm(\"C4\") (x)\n",
    "                    else:\n",
    "                        conv = layers.BatchNormalization() (x)\n",
    "                    conv = Activation(\"relu\") (conv)\n",
    "                else:\n",
    "                    conv = Activation(\"relu\") (x)\n",
    "                if is_gconv:\n",
    "                    conv = GConv2D(growth_rate*4, 1, gconv_type, gconv_type, padding=padding, kernel_regularizer=l2(weight_decay))(conv)\n",
    "                else:\n",
    "                    conv = layers.Conv2D(growth_rate*4, (1,1), padding=padding, kernel_regularizer=l2(weight_decay)) (conv)\n",
    "                if norm:\n",
    "                    if is_gconv:\n",
    "                        conv = GBatchNorm(\"C4\") (conv)\n",
    "                    else:\n",
    "                        conv = layers.BatchNormalization() (conv)\n",
    "            else:\n",
    "                if norm:\n",
    "                    if is_gconv:\n",
    "                        conv = GBatchNorm(\"C4\") (x)\n",
    "                    else:\n",
    "                        conv = layers.BatchNormalization() (x)\n",
    "                    conv = Activation(\"relu\") (conv)\n",
    "                else:\n",
    "                    conv = Activation(\"relu\") (x)\n",
    "\n",
    "            if is_gconv:\n",
    "                conv = GConv2D(growth_rate, 3, gconv_type, gconv_type, padding=padding, kernel_regularizer=l2(weight_decay))(conv)\n",
    "            else:\n",
    "                conv = layers.Conv2D(growth_rate, (3,3), padding=padding, kernel_regularizer=l2(weight_decay)) (conv)\n",
    "            if dropout > 0:\n",
    "                conv = layers.Dropout(dropout) (conv)\n",
    "\n",
    "            x = concatenate([crop(x,conv),conv])\n",
    "\n",
    "            nonlocal growable_filters\n",
    "            growable_filters += growth_rate\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def transition_block(input_layer,filters,is_gconv=True,gconv_type=\"C4\",\n",
    "                     padding=\"valid\",pooling=\"avg\",norm=True,weight_decay=1e-4,bc_model=False):\n",
    "    \n",
    "        \"\"\" \n",
    "        Used by its parent function to create individual transition blocks for use between dense blocks and\n",
    "        reduce the dimensionality of the network.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        x = input_layer\n",
    "\n",
    "        #Bottleneck and compression model of DenseNet.\n",
    "        nonlocal growable_filters\n",
    "        if bc_model:\n",
    "            filters = int(round(growable_filters*0.5))\n",
    "        else:\n",
    "            filters = growable_filters\n",
    "\n",
    "        if norm:\n",
    "            if is_gconv:\n",
    "                x = GBatchNorm(\"C4\") (x)\n",
    "            else:\n",
    "                x = layers.BatchNormalization() (x)\n",
    "\n",
    "        x = Activation(\"relu\") (x)\n",
    "\n",
    "        if is_gconv:\n",
    "            x = GConv2D(filters, 1, gconv_type, gconv_type, padding=padding, kernel_regularizer=l2(weight_decay)) (x)\n",
    "        else:\n",
    "            x = layers.Conv2D(filters, (1,1), padding=padding, kernel_regularizer=l2(weight_decay)) (x)\n",
    "\n",
    "        if pooling == \"avg\":\n",
    "            x = layers.AveragePooling2D((2,2)) (x)\n",
    "        elif pooling == \"max\":\n",
    "            x = layers.MaxPooling2D((2,2)) (x)\n",
    "        return x\n",
    "    \n",
    "    # Start of model building process\n",
    "    \n",
    "    if growable_filters==None:\n",
    "        growable_filters = filters\n",
    "    \n",
    "    input_tensor = Input(shape=(img_size,img_size,3))\n",
    "    if is_gconv:\n",
    "        x = GConv2D(filters, 3, \"Z2\", gconv_type, activation=\"relu\", padding=padding,  kernel_regularizer=l2(weight_decay)) (input_tensor)\n",
    "    else:\n",
    "        x = layers.Conv2D(filters, (3,3), activation=\"relu\", padding=padding,  kernel_regularizer=l2(weight_decay)) (input_tensor)\n",
    "    \n",
    "    for i in range(dense_blocks-1): # -1 because the final block doesn't have a transition.\n",
    "        x = dense_block(x,filters,growth_rate,is_gconv,gconv_type,conv_layers,padding,dropout,norm,weight_decay,bc_model)\n",
    "        x = transition_block(x,filters,is_gconv,gconv_type,padding,pooling,norm,weight_decay,bc_model)\n",
    " \n",
    "    # Final dense block doesn't have a transition block after it.\n",
    "    x = dense_block(x,filters,growth_rate,is_gconv,gconv_type,conv_layers,padding,dropout,norm,weight_decay,bc_model)\n",
    "    \n",
    "    if norm:\n",
    "        if is_gconv:\n",
    "            x = GBatchNorm(gconv_type) (x)\n",
    "        else:\n",
    "            x = layers.BatchNormalization() (x)\n",
    "        \n",
    "    x = Activation(\"relu\") (x)\n",
    "    \n",
    "    # If it's a Gconv model, we need to group pool before the global pooling to maintain equivariance.\n",
    "    if is_gconv:\n",
    "        x = GroupPool(h_input=gconv_type)(x)\n",
    "    \n",
    "    #Global pooling to replace the flattening layer and any dense layers prior to classification.\n",
    "    x = layers.GlobalAveragePooling2D() (x)\n",
    "    \n",
    "    output_tensor = layers.Dense(labels, activation = output) (x)\n",
    "    model = Model(input_tensor,output_tensor)\n",
    "    \n",
    "    #Compiling the model\n",
    "    if labels==1:\n",
    "        lossfunc = \"binary_crossentropy\"\n",
    "    else:\n",
    "        lossfunc = \"categorical_crossentropy\"\n",
    "    \n",
    "    if opt == \"RMS\":\n",
    "        model.compile(optimizer = optimizers.RMSprop(lr = learn),\n",
    "            loss = lossfunc,\n",
    "            metrics = metrics)\n",
    "    elif opt == \"Adam\":\n",
    "        model.compile(optimizers.Adam(lr = learn),\n",
    "            loss = lossfunc,\n",
    "            metrics = metrics)\n",
    "    elif opt == \"DenseSGD\":\n",
    "        # Using the specific settings described in the DenseNet paper.\n",
    "        model.compile(optimizer = optimizers.SGD(lr = 0.1,momentum=0.9,nesterov=True),\n",
    "                     loss = lossfunc,\n",
    "                     metrics = metrics)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping convolutional layers\n",
    "##### Required to concatenate two varying sized conv layers. Needed for the models in the rotational paper as they use \"valid\" padding.\n",
    "##### This function was adapted from https://github.com/basveeling/keras-gcnn/tree/master/keras_gcnn - access date: 26/02/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function was adapted from https://github.com/basveeling/keras-gcnn/tree/master/keras_gcnn\n",
    "def crop(inp,to_fit):\n",
    "    \n",
    "    input_layer = inp\n",
    "    skip_size = K.int_shape(input_layer)[1]\n",
    "    out_size = K.int_shape(to_fit)[1]\n",
    "    if skip_size > out_size:\n",
    "        size_diff = (skip_size - out_size) // 2\n",
    "        size_diff_odd = ((skip_size - out_size) // 2) + ((skip_size - out_size) % 2)\n",
    "        input_layer = Cropping2D(((size_diff, size_diff_odd),) * 2)(input_layer)\n",
    "    return input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting to generator <a class=\"anchor\" id=\"fitting\"></a>\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_to_generator(model,generator, x, y, vx, vy, callbacks=None, steps=None, batch_size=32, epochs = 25, validation_steps = 50, verbose = 1):\n",
    "    if steps==None:\n",
    "        steps=len(x)//batch_size\n",
    "    return model.fit_generator(\n",
    "        generator=generator.flow(x,y,batch_size),\n",
    "        steps_per_epoch = steps,\n",
    "        epochs = epochs,\n",
    "        validation_data = (vx,vy),\n",
    "        callbacks = callbacks,\n",
    "        verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_to_directory_generator(model, generator, direct, val_gen, tar_size=(96,96), class_mode=\"binary\", batch_size=32, callbacks=None, steps=100, val_steps=None, epochs = 25, verbose = 1):\n",
    "    if val_steps==None:\n",
    "        val_steps=len(val_gen)-1\n",
    "    return model.fit_generator(\n",
    "        generator.flow_from_directory(direct,\n",
    "                                      target_size=tar_size,\n",
    "                                      batch_size=batch_size,\n",
    "                                      class_mode=class_mode),        \n",
    "        steps_per_epoch = steps,\n",
    "        epochs = epochs,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps = val_steps,\n",
    "        callbacks = callbacks,\n",
    "        verbose = verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Results <a class=\"anchor\" id=\"plotting\"></a>\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adapted from my own coursework for Artificial Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(historyID,metric=\"acc\",start=0):\n",
    "    \"\"\"Plotting the metric results of Keras history objects using matplotlib.\n",
    "\n",
    "    This method takes a Keras history object and plots either the loss (blue circles) and validation loss (blue line)\n",
    "    or the accuracy (blue circles) and validation accuracy (blue line). It can start from non-zero on the x axis should\n",
    "    the user want to localise their results.\n",
    "    \n",
    "    In addition it plots the smoothed version of the validation set alongside for a clearer view of the curve.\n",
    "\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "\n",
    "    history_dict = historyID.history\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "    x = list(range(start+1,len(history_dict[\"val_loss\"])+start+1))\n",
    "    \n",
    "    if metric == \"acc\":\n",
    "        ymax = max(history_dict[\"val_acc\"])\n",
    "        xmax = history_dict[\"val_acc\"].index(ymax)\n",
    "\n",
    "        ax[0].plot(x,history_dict[\"val_acc\"][start:], \"b\", label =\"Validation acc\")\n",
    "        ax[0].plot(x,history_dict[\"acc\"][start:],\"bo\", label = \"Training acc\")\n",
    "        ax[0].set_xlabel(\"Epochs\")\n",
    "        ax[0].set_ylabel(\"Accuracy\")\n",
    "        ax[0].set_title(\"Training and validation accuracy\")\n",
    "        ax[0].legend()\n",
    "\n",
    "        anno_text = \"Highest val accuracy: x= {}, y= {}\".format(xmax, ymax)\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plot_smooth(historyID,metric=\"val_acc\")\n",
    "        \n",
    "        \n",
    "    elif metric == \"loss\":\n",
    "        ymin = min(history_dict[\"val_loss\"])\n",
    "        xmin = history_dict[\"val_loss\"].index(ymin)\n",
    "    \n",
    "        ax[0].plot(x,history_dict[\"val_loss\"][start:], \"b\", label =\"Validation loss\")\n",
    "        ax[0].plot(x,history_dict[\"loss\"][start:],\"bo\", label = \"Training loss\")\n",
    "        ax[0].set_xlabel(\"Epochs\")\n",
    "        ax[0].set_ylabel(\"Loss\")\n",
    "        ax[0].set_title(\"Training and validation loss\")\n",
    "        ax[0].legend()\n",
    "        \n",
    "        anno_text = \"Lowest val loss: x= {}, y= {}\".format(xmin, ymin)\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plot_smooth(historyID,metric=\"val_loss\")      \n",
    "        \n",
    "    else:\n",
    "        print(\"Please enter either 'acc', or 'loss' as the second parameter.\")\n",
    "        return\n",
    "        \n",
    "\n",
    "    plt.show()\n",
    "    print(anno_text)\n",
    "    \n",
    "def plot_graphs(historyID,start=0):\n",
    "    plot_results(historyID,\"acc\",start)\n",
    "    plot_results(historyID,\"loss\",start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smooth(historyID, factor = 0.9, metric = \"val_acc\"):  # ACC as primary metric currently\n",
    "    smoothed_points = []\n",
    "    for point in historyID.history[metric]:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    plt.plot(range(1, len(smoothed_points) +1), smoothed_points)\n",
    "    plt.title(\"smooth \" + metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    return plt\n",
    "\n",
    "def plot_smooth_graphs(historyID, factor = 0.9):\n",
    "    plot_smooth(historyID, factor, metric=\"val_acc\")\n",
    "    plot_smooth(historyID, factor, metric=\"val_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC/ROC calculation and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilising the SKlearn ROC_curve metric and basing the plot off of https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def evaluate_auc(model,generator,batches=None):\n",
    "    if batches==None:\n",
    "        batches = len(generator)-1\n",
    "    predictions = []\n",
    "    test_labels = []\n",
    "    count = 0\n",
    "    for img_batch,label_batch in generator:\n",
    "        test_labels.extend(label_batch)\n",
    "        predictions.extend(model.predict(img_batch))\n",
    "        count+=1\n",
    "        if count >= batches:\n",
    "            break\n",
    "    plot_auc(test_labels,predictions)\n",
    "        \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html - based off the example shown.\n",
    "def plot_auc(y,y_pred):\n",
    "    plt.clf()\n",
    "             \n",
    "    dashed_green_line = \"g--\"\n",
    "    fig, ax = plt.subplots(1,1, figsize=(7,5))\n",
    "    \n",
    "    false_positive_rate, true_positive_rate = roc_curve(y,y_pred)[:2]\n",
    "    label = \"AUC: %.3f\" % area_under_ROC_curve(false_positive_rate,true_positive_rate)\n",
    "    ax.plot(false_positive_rate,true_positive_rate, color = \"red\",label=label)\n",
    "    ax.plot([0,1],[0,1],dashed_green_line)\n",
    "    ax.fill_between(ax.lines[0].get_xydata()[:,0],ax.lines[0].get_xydata()[:,1],color = \"cyan\",alpha=0.2)\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.05])\n",
    "    ax.legend(loc=4)\n",
    "    ax.set_ylabel(\"True positive rate\")\n",
    "    ax.set_xlabel(\"False positive rate\")\n",
    "    ax.set_title(\"Receiver Operating Characteristic Curve\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def area_under_ROC_curve(false_positive_rate,true_positive_rate):\n",
    "    # Using the trapezoid rule for calculating area\n",
    "    # based off https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
    "    # and discussed in http://www.med.mcgill.ca/epidemiology/hanley/software/Hanley_McNeil_Radiology_82.pdf\n",
    "    return np.trapz(true_positive_rate,false_positive_rate)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous <a class=\"anchor\" id=\"misc\"></a>\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to check whether the model is rotationally equivariant.\n",
    "# It generates predictions on a sample from the training set and a rotated version of that sample.\n",
    "# If these return the same result, the model is equivariant.\n",
    "\n",
    "# Concatenates the image and its rotated variants into one numpy array.\n",
    "# Then passes the array through the model for predictions\n",
    "# and checks whether all predictions are virtually identical.\n",
    "\n",
    "def equi_check(model,X):\n",
    "    # Concatenating the image and its rotated variant into 1 numpy array.\n",
    "    imgs = np.array([X[0], np.rot90(X[0]),np.rot90(np.rot90(X[0])),np.rot90(np.rot90(np.rot90(X[0])))])\n",
    "    # Passing the array through the model for predictions\n",
    "    predictions = model.predict(imgs)\n",
    "    # Checking whether all predictions are virtually identical.\n",
    "    return np.allclose(predictions[0], predictions[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model memory required \n",
    "#####  Adapted from https://stackoverflow.com/questions/43137288/how-to-determine-needed-memory-of-keras-model - access date: 27/02/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Adapted from https://stackoverflow.com/questions/43137288/how-to-determine-needed-memory-of-keras-model \n",
    "def memory_required(model,b_size):\n",
    "    shapes_count = int(np.sum([np.prod(np.array([s if isinstance(s, int) else 1 for s in l.output_shape])) for l in model.layers]))\n",
    "    memory = shapes_count * 4\n",
    "    gbytes = np.round(memory * b_size / (1024.0 ** 3), 3)\n",
    "    return (gbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting the convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_conv_layers(model):\n",
    "    sum=0\n",
    "    for l in model.layers:\n",
    "        if \"conv\" in l.get_config()[\"name\"]:\n",
    "            sum+=1\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading the history object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_history(history,history_string):\n",
    "    pickle_out = open(\"C:/GitRepos/FINAL PROJECT DATA/Histopathologic Cancer Detection/histories/{}.pickle\".format(history_string),\"wb\")\n",
    "    pickle.dump(history,pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "def load_history(history_string):\n",
    "    pickle_in = open(\"C:/GitRepos/FINAL PROJECT DATA/Histopathologic Cancer Detection/histories/{}.pickle\".format(history_string),\"rb\")\n",
    "    return pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ensembling <a class=\"anchor\" id=\"ensembling\"></a>\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluates the ensemble and the original models for comparison.\n",
    "def model_ensemble_evaluation(models,test_gen,steps=None,weighted=False):\n",
    "    \"\"\" Takes a list of models, a data generator and optional weighting as input.\n",
    "     Ensembles the models and performed rank-averaging (if chosen) weighting on them based off a small\n",
    "     validation set. It then makes predictions on the test set and pools them together.    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if steps==None:\n",
    "        steps = len(test_gen)-1\n",
    "    \n",
    "    test_sample = []\n",
    "    test_labels_sample = []\n",
    "    val_sample = []\n",
    "    val_labels_sample = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        next_batch = test_gen.next()\n",
    "        if weighted and i<steps*0.2:\n",
    "            val_sample.extend(next_batch[0])\n",
    "            val_labels_sample.extend(next_batch[1])\n",
    "        else:\n",
    "            test_sample.extend(next_batch[0])\n",
    "            test_labels_sample.extend(next_batch[1])\n",
    "   \n",
    "    #converting to numpy array\n",
    "    test_sample = np.array(test_sample)\n",
    "    test_labels_sample=np.array(test_labels_sample)\n",
    "    val_sample = np.array(val_sample)\n",
    "    val_labels_sample = np.array(val_labels_sample)\n",
    "    \n",
    "    pred_list = []\n",
    "    for m in models:\n",
    "        pred_list.append(m.predict(test_sample))\n",
    "    \n",
    "    # If not utilising the weighted predictions, simply average them out.\n",
    "    if not weighted:\n",
    "        ensembled_pred = np.sum(pred_list,axis=0)/len(models)\n",
    "    else:\n",
    "        val_scores = []\n",
    "        for m in models:\n",
    "            # For each model, evaluate a validation score.\n",
    "            val_scores.append(m.evaluate(val_sample,val_labels_sample)[1])\n",
    "        val_sum = np.sum(val_scores)\n",
    "        print(\"val scores: \", val_scores)\n",
    "        weights = []\n",
    "        for i in val_scores:\n",
    "        # Normalise the weights based on the validation scores.\n",
    "            weights.append(i/val_sum)\n",
    "        print(\"weights\",weights)\n",
    "        ensembled_pred = np.zeros((len(test_labels_sample),1))\n",
    "        for count, p in enumerate(pred_list):\n",
    "            # Sum up the weighted predictions.\n",
    "            ensembled_pred += p*weights[count]     \n",
    "    \n",
    "    \n",
    "    pred_list.append(ensembled_pred)\n",
    "    return acc_comparison(pred_list,test_labels_sample)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "def acc_comparison(predictions_list,labels):\n",
    "    accuracies=[]\n",
    "    AUCs=[]\n",
    "    for preds in predictions_list:\n",
    "        false_positive_rate, true_positive_rate = roc_curve(labels,preds)[:2]\n",
    "        AUCs.append(area_under_ROC_curve(false_positive_rate,true_positive_rate))\n",
    "        correct = 0\n",
    "        for i in range(len(labels)):\n",
    "            if int(round(preds[i][0])) == labels[i]:\n",
    "                correct+=1\n",
    "        acc = correct/len(labels)\n",
    "        accuracies.append(acc)\n",
    "    return accuracies,AUCs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
